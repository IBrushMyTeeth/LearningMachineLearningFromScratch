# K-Nearest Neighbors (KNN)

## Overview
This project explores **K-Nearest Neighbors (KNN) from scratch**, using only **NumPy**. The goal is to understand how KNN works conceptually and practically, and to investigate model behavior through **bias-variance analysis** and performance evaluation.

The implementation avoids external ML libraries, giving full visibility into how distances are computed, neighbors are selected, and predictions are made. Additionally, experiments include **bootstrapping data** and simulating different datasets to study the robustness of KNN.

## Key Learnings
- Implemented KNN **fully with NumPy**, including distance calculations, neighbor selection, and voting.  
- Explored **model evaluation** using **confusion matrices**.  
- Studied **bias-variance tradeoff** by varying the number of neighbors and dataset characteristics.  
- Applied **bootstrapping** and dataset simulation to understand model sensitivity and variance.  
- Gained intuition on **distance metrics**, **hyperparameter selection**, and their impact on generalization.  

## Next Steps
- Extend the implementation to **weighted KNN** based on distances.  
- Explore **different distance metrics** (Manhattan, Minkowski) for performance comparison.  
- Integrate visualization tools to better illustrate **decision boundaries**.

